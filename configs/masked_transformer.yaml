# ===============================
# Masked Latent Transformer Config
# ===============================

# ---------- data ----------
train_latents: /local/altamabp/audience_tuning-gem/vqvae/AUD-91/latents/train_latents.npy
train_indices: /local/altamabp/audience_tuning-gem/vqvae/AUD-91/latents/train_indices.npy
train_labels:  /local/altamabp/audience_tuning-gem/vqvae/AUD-91/latents/train_labels.npy
train_probs:   /local/altamabp/audience_tuning-gem/vqvae/AUD-91/latents/train_probs.npy

val_latents:   /local/altamabp/audience_tuning-gem/vqvae/AUD-91/latents/val_latents.npy
val_indices:   /local/altamabp/audience_tuning-gem/vqvae/AUD-91/latents/val_indices.npy
val_labels:    /local/altamabp/audience_tuning-gem/vqvae/AUD-91/latents/val_labels.npy
val_probs:     /local/altamabp/audience_tuning-gem/vqvae/AUD-91/latents/val_probs.npy

# ---------- model ----------
vocab_size: 32
num_layers: 6
num_heads: 3
hidden_dim: 512
dropout: 0.1

# ---------- conditioning ----------
num_classes: 10
label_dim: 16
p_use_probs: 0.0 #0.5 # probability of using classifier probs/logits instead of labels (float32 in [0,1], default 0.0) / 0.0 → always labels / 1.0 → always probs / 0.5 → half/half
# conditioning mode: 
# 0.0 = labels only
# 1.0 = classifier probs only
# 0.0 < p < 1.0 = mixed (recommended) e.g: 0.25 → more label-driven / 0.75 → more classifier-driven

# ---------- training ----------
bs: 16
epochs: 100
lr: 3.0e-4
mask_prob: 0.5
loss_masked: true
scheduler: cosine
warmup_steps: 1000
seed: 42
num_workers: 4

# ---------- misc ----------
run_dir: ./runs
torch_compile: false
